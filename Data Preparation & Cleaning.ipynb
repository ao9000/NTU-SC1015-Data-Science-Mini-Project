{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd7dcf7",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46336ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aozy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aozy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aozy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For text processing, cleaning\n",
    "import nltk\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import text2emotion as te\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc46404",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106d7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "news_data = pd.read_csv(\"train.csv\")\n",
    "news_data.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf80615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6877f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title              author  \\\n",
       "id                                                                          \n",
       "0   House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2                   Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                 text  label  \n",
       "id                                                            \n",
       "0   House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1   Ever get the feeling your life circles the rou...      0  \n",
       "2   Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3   Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4   Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315b3e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20800 entries, 0 to 20799\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   20242 non-null  object\n",
      " 1   author  18843 non-null  object\n",
      " 2   text    20761 non-null  object\n",
      " 3   label   20800 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 812.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Information about the Variables\n",
    "news_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e17264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title     object\n",
       "author    object\n",
       "text      object\n",
       "label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0650d3",
   "metadata": {},
   "source": [
    "# Data Cleaning (Handling of null rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93257e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if any of the columns contains NULL value\n",
    "news_data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aef03aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text null rows, replace null with title\n",
    "news_data.loc[news_data[\"text\"].isnull() & ~news_data[\"title\"].isnull(),'text'] = news_data[\"title\"]\n",
    "\n",
    "# For title null rows, get first 50 characters of text as title\n",
    "news_data.loc[news_data[\"title\"].isnull() & ~news_data[\"text\"].isnull(),'title'] = news_data[\"text\"].str[:50]\n",
    "\n",
    "# For the null authors, assign \"Unknown\" as author\n",
    "news_data.loc[news_data[\"author\"].isnull(), 'author'] = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6993535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rest of the rows with NULL values\n",
    "news_data.dropna(inplace=True)\n",
    "\n",
    "# Check how many rows remain\n",
    "news_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556aaf5",
   "metadata": {},
   "source": [
    "## Some data standardization before EDA (For better presentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cd13420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_translated</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>not fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title              author  \\\n",
       "id                                                                          \n",
       "0   House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2                   Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                 text  label label_translated  \n",
       "id                                                                             \n",
       "0   House Dem Aide: We Didn’t Even See Comey’s Let...      1             fake  \n",
       "1   Ever get the feeling your life circles the rou...      0         not fake  \n",
       "2   Why the Truth Might Get You Fired October 29, ...      1             fake  \n",
       "3   Videos 15 Civilians Killed In Single US Airstr...      1             fake  \n",
       "4   Print \\nAn Iranian woman has been sentenced to...      1             fake  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new column values to real/fake classes\n",
    "news_data['label_translated'] = np.where(news_data['label']==1, 'fake', 'not fake')\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621bf945",
   "metadata": {},
   "source": [
    "# Generate extra data using text processing techniques (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379bac9",
   "metadata": {},
   "source": [
    "## Character & Word count of Title & Text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76ff8399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_translated</th>\n",
       "      <th>title_charcount</th>\n",
       "      <th>text_charcount</th>\n",
       "      <th>title_wordcount</th>\n",
       "      <th>text_wordcount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>81</td>\n",
       "      <td>4930</td>\n",
       "      <td>14</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>not fake</td>\n",
       "      <td>55</td>\n",
       "      <td>4160</td>\n",
       "      <td>9</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>33</td>\n",
       "      <td>7692</td>\n",
       "      <td>7</td>\n",
       "      <td>1266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>63</td>\n",
       "      <td>3237</td>\n",
       "      <td>10</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>93</td>\n",
       "      <td>938</td>\n",
       "      <td>14</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title              author  \\\n",
       "id                                                                          \n",
       "0   House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2                   Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                 text  label label_translated  \\\n",
       "id                                                                              \n",
       "0   House Dem Aide: We Didn’t Even See Comey’s Let...      1             fake   \n",
       "1   Ever get the feeling your life circles the rou...      0         not fake   \n",
       "2   Why the Truth Might Get You Fired October 29, ...      1             fake   \n",
       "3   Videos 15 Civilians Killed In Single US Airstr...      1             fake   \n",
       "4   Print \\nAn Iranian woman has been sentenced to...      1             fake   \n",
       "\n",
       "    title_charcount  text_charcount  title_wordcount  text_wordcount  \n",
       "id                                                                    \n",
       "0                81            4930               14             820  \n",
       "1                55            4160                9             710  \n",
       "2                33            7692                7            1266  \n",
       "3                63            3237               10             557  \n",
       "4                93             938               14             154  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column for the number of char for title and text\n",
    "news_data['title_charcount'] = news_data['title'].str.len()\n",
    "news_data['text_charcount'] = news_data['text'].str.len()\n",
    "\n",
    "# Add a column for the number of word for title and text\n",
    "news_data['title_wordcount'] = news_data['title'].str.split().str.len()\n",
    "news_data['text_wordcount'] = news_data['text'].str.split().str.len()\n",
    "\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd9412",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8721cfb6",
   "metadata": {},
   "source": [
    "## Removal of unnecessary data (Symbols & Numbers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e241ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove symbols & numbers (Non useful data)\n",
    "def remove_symbols_numbers(string):\n",
    "    # Use regex to replace for anything that is not alphabets and punctuation marks\n",
    "    string = re.sub('[^a-zA-Z!?\\']', ' ', string)\n",
    "    # Eliminate multiple spaces\n",
    "    return \" \".join(string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fabae67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all functions to whole data\n",
    "news_data['text'] = news_data['text'].apply(remove_symbols_numbers)\n",
    "# Do the same for title\n",
    "news_data['title'] = news_data['title'].apply(remove_symbols_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11caaf7d",
   "metadata": {},
   "source": [
    "## Removal of stopwords & generate stopwords count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a71cf6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'haven', 'for', \"she'd\", \"we're\", 'why', 't', 'most', 'some', 'are', \"weren't\", 'but', \"isn't\", 'herself', 'between', 'out', 'into', 'few', 'their', 'each', 'below', 'her', 'what', 'through', 'm', 'until', 'at', 've', \"you'd\", 'where', 'o', \"who's\", 'all', \"hadn't\", 'isn', 'were', 'wasn', \"wouldn't\", \"that's\", 'by', 'doesn', 'under', 's', 'being', 'had', 'further', 'won', \"we'll\", 'd', 'weren', 'very', 'there', 'those', 'because', 'and', 'shan', \"aren't\", \"what's\", \"you're\", 'hadn', 'too', 'ma', 'shouldn', 'here', 'am', 'been', 'as', 'didn', 'on', 'how', 'once', 'just', 'so', \"you've\", 'should', 'do', \"she'll\", \"didn't\", 'your', 'after', 'mightn', 'would', 'could', 'ours', \"shan't\", \"it's\", 'we', 'can', 'did', 'be', 'my', \"he's\", \"they'll\", 'over', 'if', 'down', 'other', 'me', 'mustn', \"can't\", \"haven't\", \"how's\", \"don't\", 'before', \"doesn't\", 'that', \"let's\", 'himself', 'you', \"wasn't\", 'itself', 'while', 'against', 'only', 'these', 'does', 'having', 'aren', \"they've\", \"i'll\", \"needn't\", 'to', 'yourself', 'was', 'from', 'don', 'theirs', \"hasn't\", 'than', 'needn', 'couldn', 'then', 'nor', \"i've\", 'll', 'myself', \"i'm\", 'them', \"they'd\", 'him', 'ourselves', 'a', 'or', 'not', 'his', 'yours', 'has', 'the', 'more', 'hasn', 'its', \"should've\", 'which', 'they', 'y', 'i', 'have', 'no', \"they're\", 'themselves', \"i'd\", 'when', \"won't\", \"there's\", \"you'll\", 'now', 'whom', 'during', 'of', 'this', \"mightn't\", \"where's\", 'same', 'about', 'in', 'such', 'wouldn', \"she's\", \"we've\", 'doing', \"why's\", \"here's\", 'off', 'with', \"that'll\", 'cannot', \"he'd\", 'she', 'up', 'it', 'is', 'both', 'will', \"mustn't\", 'who', 'ain', \"he'll\", 're', 'own', 'hers', 'yourselves', 'above', \"couldn't\", 'again', 'he', 'our', \"shouldn't\", 'ought', 'an', \"we'd\", 'any', \"when's\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aozy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Assign to variable\n",
    "my_stopwords = stopwords.words('english')\n",
    "\n",
    "# Add some additional stopwords\n",
    "my_stopwords.extend([\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \n",
    "                     \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \n",
    "                     \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \n",
    "                     \"doing\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\", \"has\", \n",
    "                     \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \n",
    "                     \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \n",
    "                     \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \n",
    "                     \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \n",
    "                     \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \n",
    "                     \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \n",
    "                     \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \n",
    "                     \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \n",
    "                     \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \n",
    "                     \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "                    ])\n",
    "# Set to remove duplicates and re-assign to variable\n",
    "# Print to see total list of stopwords\n",
    "print(my_stopwords:=set(my_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67d488f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords (Words that do not give any meaning)\n",
    "def remove_stop_words(string):\n",
    "    # Convert to lowercase for stopwords matching\n",
    "    # Tokenize sentence into list\n",
    "    char_arr = string.lower().split()\n",
    "    # Record the number of stopwords deleted\n",
    "    count = sum(char in my_stopwords for char in char_arr)\n",
    "    # Iterate list of words, only keeping words that are not stopwords\n",
    "    char_arr = [char for char in char_arr if not char in my_stopwords]\n",
    "    # Form the sentence back\n",
    "    return \" \".join(char_arr), count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7543ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all functions to whole data\n",
    "news_data['text'], news_data['stopwords_count_text'] = zip(*news_data['text'].apply(remove_stop_words))\n",
    "# Do the same for title\n",
    "news_data['title'], news_data['stopwords_count_title'] = zip(*news_data['title'].apply(remove_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3007908",
   "metadata": {},
   "source": [
    "## Word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a6f2d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init stemming object\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Function to perform stemming (Reducing words to their root form)\n",
    "def stem(string):\n",
    "    # Tokenize sentence into list\n",
    "    char_arr = string.split()\n",
    "    # Iterate list of words, stemming each word\n",
    "    char_arr = [ps.stem(char) for char in char_arr]\n",
    "    # Form the sentence back\n",
    "    return \" \".join(char_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d6477b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all functions to whole data\n",
    "news_data['text'] = news_data['text'].apply(stem)\n",
    "# Do the same for title\n",
    "news_data['title'] = news_data['title'].apply(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0b7bf",
   "metadata": {},
   "source": [
    "# Generate extra data using text processing techniques (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd27a4",
   "metadata": {},
   "source": [
    "## Generate sentiment (Polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2b72a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Aozy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download resources needed\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Init SIA object\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to return the full form of the polarity\n",
    "def format_polarity(polarity):\n",
    "    # Return its proper form\n",
    "    if polarity == \"neu\":\n",
    "        return \"neutral\"\n",
    "    elif polarity == \"pos\":\n",
    "        return \"positive\"\n",
    "    elif polarity == \"neg\":\n",
    "        return \"negative\"\n",
    "\n",
    "# Function to return the polarity & the corresponding confidence score\n",
    "def get_polarity_score(sentence):\n",
    "    # Check if sentence is very long (Over 100 words) (Takes too long to process many words, therefore limit to 100)\n",
    "    if len(sentence.split()) > 100:\n",
    "        # Only take the first 100 words\n",
    "        sentence = \" \".join(sentence.split()[:100])\n",
    "    \n",
    "    # Get polarity & score in dict\n",
    "    polarity_score = sia.polarity_scores(sentence)\n",
    "    # Delete compound as we do not require it\n",
    "    del polarity_score['compound']\n",
    "    # Sort to highest polarity and get its score\n",
    "    polarity, score = sorted(polarity_score.items(), key=lambda x: x[1], reverse=True)[:1][0]\n",
    "    # Format polarity into proper form\n",
    "    polarity = format_polarity(polarity)\n",
    "    return polarity, score\n",
    "\n",
    "# Add 2 columns polarity & score (Title)\n",
    "news_data['title_polarity'], news_data['title_polarity_score'] = zip(*news_data['title'].apply(get_polarity_score))\n",
    "                                                       \n",
    "# Add 2 columns polarity & score (Text)\n",
    "news_data['text_polarity'], news_data['text_polarity_score'] = zip(*news_data['text'].apply(get_polarity_score))                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc8136",
   "metadata": {},
   "source": [
    "## Generate emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d9d0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the corresponding emotions for any sentence\n",
    "def get_emotion(sentence):\n",
    "    # Check if sentence is very long (Over 30 words) (Takes too long to process many words, therefore limit to 30)\n",
    "    if len(sentence.split()) > 30:\n",
    "        # Only take the first 30 words\n",
    "        sentence = \" \".join(sentence.split()[:30])\n",
    "    \n",
    "    # Get emotion\n",
    "    emotion_dict = te.get_emotion(sentence)\n",
    "    # Filter to highest scoring emotion\n",
    "    emotion, score = sorted(emotion_dict.items(), key=lambda x: x[1], reverse=True)[:1][0]\n",
    "    # Check if emotion is balanced, therefore neutral\n",
    "    if score == 0.0:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return emotion\n",
    "    \n",
    "# Add 1 column emotion (Title)\n",
    "news_data['title_emotion'] = news_data['title'].apply(get_emotion)\n",
    "                                                       \n",
    "# Add 1 column emotion (text)\n",
    "news_data['text_emotion'] = news_data['text'].apply(get_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c8072",
   "metadata": {},
   "source": [
    "## Parts of speech tagging (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a901dbb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aozy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Aozy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download required resources for POS\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# Function to return new columns for POS count for any sentence\n",
    "def pos_tagging(sentence):\n",
    "    # Create temp dict\n",
    "    pos_dict = {}\n",
    "    # Tokenize sentence\n",
    "    sentence_token = sentence.split()\n",
    "    # Get pos tagging in list\n",
    "    pos_list = nltk.pos_tag(sentence_token)\n",
    "    # Simplify\n",
    "    pos_list = [map_tag('en-ptb', 'universal', tag) for _, tag in pos_list]\n",
    "    pos_dict = Counter(pos_list)\n",
    "    return [pos_dict['ADJ'] if 'ADJ' in pos_dict else 0, \n",
    "            pos_dict['ADV'] if 'ADV' in pos_dict else 0, \n",
    "            pos_dict['NOUN'] if 'NOUN' in pos_dict else 0,\n",
    "            pos_dict['NUM'] if 'NUM' in pos_dict else 0,\n",
    "            pos_dict['PRON'] if 'PRON' in pos_dict else 0,\n",
    "            pos_dict['VERB'] if 'VERB' in pos_dict else 0]\n",
    "\n",
    "# Add 1 column pos (Title)\n",
    "news_data['title_pos_adj'], news_data['title_pos_adv'], news_data['title_pos_noun'], news_data['title_pos_num'], news_data['title_pos_pron'], news_data['title_pos_verb'] = zip(*news_data['title'].apply(pos_tagging))\n",
    "                                                       \n",
    "# Add 1 column pos (text)\n",
    "news_data['text_pos_adj'], news_data['text_pos_adv'], news_data['text_pos_noun'], news_data['text_pos_num'], news_data['text_pos_pron'], news_data['text_pos_verb'] = zip(*news_data['text'].apply(pos_tagging))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2310b8",
   "metadata": {},
   "source": [
    "# Final cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd18ac2",
   "metadata": {},
   "source": [
    "## After cleaning, delete rows which are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7b4f3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:\t\t\t130\n",
      "author:\t\t\t0\n",
      "text:\t\t\t102\n",
      "label:\t\t\t0\n",
      "label_translated:\t\t\t0\n",
      "title_charcount:\t\t\t0\n",
      "text_charcount:\t\t\t0\n",
      "title_wordcount:\t\t\t0\n",
      "text_wordcount:\t\t\t0\n",
      "stopwords_count_text:\t\t\t0\n",
      "stopwords_count_title:\t\t\t0\n",
      "title_polarity:\t\t\t0\n",
      "title_polarity_score:\t\t\t0\n",
      "text_polarity:\t\t\t0\n",
      "text_polarity_score:\t\t\t0\n",
      "title_emotion:\t\t\t0\n",
      "text_emotion:\t\t\t0\n",
      "title_pos_adj:\t\t\t0\n",
      "title_pos_adv:\t\t\t0\n",
      "title_pos_noun:\t\t\t0\n",
      "title_pos_num:\t\t\t0\n",
      "title_pos_pron:\t\t\t0\n",
      "title_pos_verb:\t\t\t0\n",
      "text_pos_adj:\t\t\t0\n",
      "text_pos_adv:\t\t\t0\n",
      "text_pos_noun:\t\t\t0\n",
      "text_pos_num:\t\t\t0\n",
      "text_pos_pron:\t\t\t0\n",
      "text_pos_verb:\t\t\t0\n"
     ]
    }
   ],
   "source": [
    "# Check if any of the columns are empty string \"\" after cleaning\n",
    "for col in news_data.columns:\n",
    "    # Print sum of empty strings in a col\n",
    "    print(f\"{col}:\\t\\t\\t{len(news_data[news_data[col] == ''])}\")\n",
    "    # Replace empty string with NA to clean\n",
    "    news_data[col].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b3b109f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                    130\n",
       "author                     0\n",
       "text                     102\n",
       "label                      0\n",
       "label_translated           0\n",
       "title_charcount            0\n",
       "text_charcount             0\n",
       "title_wordcount            0\n",
       "text_wordcount             0\n",
       "stopwords_count_text       0\n",
       "stopwords_count_title      0\n",
       "title_polarity             0\n",
       "title_polarity_score       0\n",
       "text_polarity              0\n",
       "text_polarity_score        0\n",
       "title_emotion              0\n",
       "text_emotion               0\n",
       "title_pos_adj              0\n",
       "title_pos_adv              0\n",
       "title_pos_noun             0\n",
       "title_pos_num              0\n",
       "title_pos_pron             0\n",
       "title_pos_verb             0\n",
       "text_pos_adj               0\n",
       "text_pos_adv               0\n",
       "text_pos_noun              0\n",
       "text_pos_num               0\n",
       "text_pos_pron              0\n",
       "text_pos_verb              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if any of the columns contains NULL value after cleaning\n",
    "news_data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2c42621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20584, 29)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rest of the rows with NULL values\n",
    "news_data.dropna(inplace=True)\n",
    "\n",
    "# Reset index for dropped columns\n",
    "news_data.reset_index(inplace=True, drop=True)\n",
    "# Set index name back to original name\n",
    "news_data.index.name = 'id'\n",
    "\n",
    "# Check how many rows remain\n",
    "news_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01deedc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_translated</th>\n",
       "      <th>title_charcount</th>\n",
       "      <th>text_charcount</th>\n",
       "      <th>title_wordcount</th>\n",
       "      <th>text_wordcount</th>\n",
       "      <th>stopwords_count_text</th>\n",
       "      <th>...</th>\n",
       "      <th>title_pos_noun</th>\n",
       "      <th>title_pos_num</th>\n",
       "      <th>title_pos_pron</th>\n",
       "      <th>title_pos_verb</th>\n",
       "      <th>text_pos_adj</th>\n",
       "      <th>text_pos_adv</th>\n",
       "      <th>text_pos_noun</th>\n",
       "      <th>text_pos_num</th>\n",
       "      <th>text_pos_pron</th>\n",
       "      <th>text_pos_verb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hous dem aid even see comey letter jason chaff...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>hous dem aid even see comey letter jason chaff...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>81</td>\n",
       "      <td>4930</td>\n",
       "      <td>14</td>\n",
       "      <td>820</td>\n",
       "      <td>406</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>16</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flynn hillari clinton big woman campu breitbart</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>ever get feel life circl roundabout rather hea...</td>\n",
       "      <td>0</td>\n",
       "      <td>not fake</td>\n",
       "      <td>55</td>\n",
       "      <td>4160</td>\n",
       "      <td>9</td>\n",
       "      <td>710</td>\n",
       "      <td>330</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>13</td>\n",
       "      <td>208</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>truth might get fire</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>truth might get fire octob tension intellig an...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>33</td>\n",
       "      <td>7692</td>\n",
       "      <td>7</td>\n",
       "      <td>1266</td>\n",
       "      <td>575</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>155</td>\n",
       "      <td>33</td>\n",
       "      <td>385</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>civilian kill singl us airstrik identifi</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>video civilian kill singl us airstrik identifi...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>63</td>\n",
       "      <td>3237</td>\n",
       "      <td>10</td>\n",
       "      <td>557</td>\n",
       "      <td>244</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iranian woman jail fiction unpublish stori wom...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>print iranian woman sentenc six year prison ir...</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>93</td>\n",
       "      <td>938</td>\n",
       "      <td>14</td>\n",
       "      <td>154</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title              author  \\\n",
       "id                                                                          \n",
       "0   hous dem aid even see comey letter jason chaff...       Darrell Lucus   \n",
       "1     flynn hillari clinton big woman campu breitbart     Daniel J. Flynn   \n",
       "2                                truth might get fire  Consortiumnews.com   \n",
       "3            civilian kill singl us airstrik identifi     Jessica Purkiss   \n",
       "4   iranian woman jail fiction unpublish stori wom...      Howard Portnoy   \n",
       "\n",
       "                                                 text  label label_translated  \\\n",
       "id                                                                              \n",
       "0   hous dem aid even see comey letter jason chaff...      1             fake   \n",
       "1   ever get feel life circl roundabout rather hea...      0         not fake   \n",
       "2   truth might get fire octob tension intellig an...      1             fake   \n",
       "3   video civilian kill singl us airstrik identifi...      1             fake   \n",
       "4   print iranian woman sentenc six year prison ir...      1             fake   \n",
       "\n",
       "    title_charcount  text_charcount  title_wordcount  text_wordcount  \\\n",
       "id                                                                     \n",
       "0                81            4930               14             820   \n",
       "1                55            4160                9             710   \n",
       "2                33            7692                7            1266   \n",
       "3                63            3237               10             557   \n",
       "4                93             938               14             154   \n",
       "\n",
       "    stopwords_count_text  ...  title_pos_noun title_pos_num  title_pos_pron  \\\n",
       "id                        ...                                                 \n",
       "0                    406  ...               6             0               0   \n",
       "1                    330  ...               5             0               0   \n",
       "2                    575  ...               2             0               0   \n",
       "3                    244  ...               2             0               1   \n",
       "4                     64  ...               7             0               0   \n",
       "\n",
       "   title_pos_verb  text_pos_adj text_pos_adv text_pos_noun  text_pos_num  \\\n",
       "id                                                                         \n",
       "0               1           102           16           234             3   \n",
       "1               1            81           13           208             3   \n",
       "2               2           155           33           385             4   \n",
       "3               1            74            5           158             6   \n",
       "4               1            13            5            54             2   \n",
       "\n",
       "    text_pos_pron  text_pos_verb  \n",
       "id                                \n",
       "0               0             68  \n",
       "1               3             45  \n",
       "2               3             92  \n",
       "3              12             44  \n",
       "4               0             11  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View cleaned data\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0a8be",
   "metadata": {},
   "source": [
    "# Save cleaned data to disk (Cleaning takes too long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2938dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file (Cleaning too long, around 1h)\n",
    "news_data.to_csv(\"cleaned_news_data.csv\", sep=',', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
